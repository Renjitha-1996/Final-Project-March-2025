{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a530d14-30fd-4761-9561-6da68428a08b",
   "metadata": {},
   "source": [
    "# Title of ML project : Telangana Regional Transport Authority Vehicle Online Sales Data 01-01-2025 to 31-01-2025\n",
    "\n",
    "# Name : Renjitha E R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20f8aeb-f351-4e0b-ae78-8e30da53663f",
   "metadata": {},
   "source": [
    "# Organization : Entri Elevate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327c2f3a-328a-49a4-a343-b6ff4b6a06c6",
   "metadata": {},
   "source": [
    "![Bus Image](/full/path/to/86419624.webp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d762ce2f-fa2e-4efb-9eb0-3f1fcc3783a9",
   "metadata": {},
   "source": [
    "# Overview of Problem Statement\n",
    "\n",
    "In 2025, a significant number of vehicles will require insurance renewal. The goal of this project is to predict the number of vehicles that need insurance renewal in 2025 using machine learning techniques. This prediction is crucial for insurance companies, government agencies, and vehicle owners to plan ahead, allocate resources efficiently, and ensure seamless insurance renewal processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4158fc43-0cd0-414e-b45e-3f92b3f0f7eb",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "Predicting the number of vehicles that need insurance renewal in 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5beaf3a-fe3e-4b65-9bb2-e9e95c20e894",
   "metadata": {},
   "source": [
    "# Data Description\n",
    "\n",
    "Source Of Data: The dataset collected from Telengana Government Site\n",
    "\n",
    "# Feature\n",
    "\n",
    "slno                   :\tSerial number of the record.\n",
    "modelDesc              :\tDescription of the vehicle model.\n",
    "fuel                   :\tType of fuel used (e.g., Petrol, Diesel, Electric).\n",
    "colour                 :\tVehicle color.\n",
    "vehicleClass           :\tCategory of the vehicle (e.g., Motor Cycle, Goods Carriage).\n",
    "makeYear               :\tThe manufacturing year of the vehicle.\n",
    "seatCapacity           :\tNumber of seats in the vehicle.\n",
    "insuranceValidity      :\tThe expiry date of the vehicle's insurance.\n",
    "secondVehicle          :\tIndicates whether this is the owner's second vehicle (Y or N).\n",
    "tempRegistrationNumber :\tTemporary registration number of the vehicle.\n",
    "category               :\tSpecifies if the vehicle is Transport or Non-Transport.\n",
    "makerName              : \tName of the vehicle manufacturer (e.g., Honda, Bajaj, Ashok Leyland).\n",
    "OfficeCd               :\tCode of the registering office (RTO).\n",
    "fromdate               :\tRegistration start date.\n",
    "to date                :\tRegistration end date."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2f9ed7-89f1-43a0-b86c-529df706156a",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "46f9b9ac-60d1-4569-89a9-aba0a1f26ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy.stats import skew\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression,Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ed68c665-7a9f-455a-8d2f-a4ccfded39b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ts_transport_online_sales_01_01_2025to31_01_2025.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[118], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mts_transport_online_sales_01_01_2025to31_01_2025.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ts_transport_online_sales_01_01_2025to31_01_2025.csv'"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df =pd.read_csv('ts_transport_online_sales_01_01_2025to31_01_2025.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7335bf48-d636-4563-a468-7941e8bde5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdc7c0b-4305-4582-bdb5-f1f1ecc61eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3b5865-0afb-4684-86d5-511eec5fb83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaaa86e-7ae5-4139-babf-c47e9e2c7644",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de86f56-c2b9-46a8-b619-b9edd9c964fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2601c0f-aa9b-4202-93d0-73e16b10594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['fuel', 'colour', 'seatCapacity'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd7f73e-669a-442a-b097-6aa1187a3627",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56491c2-fccb-4bfa-b015-54f69ebd08ca",
   "metadata": {},
   "source": [
    "# Data Preprocessing - Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cb0718-3f31-4fbf-b0a3-95a6e30ffed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'insuranceValidity' to datetime\n",
    "df['insuranceValidity'] = pd.to_datetime(df['insuranceValidity'],errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8520401-10dd-4807-b282-e4560aadc9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the year of insurance validity\n",
    "df['insuranceYear'] = df['insuranceValidity'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d7be56-b049-4cfd-9a72-b64554524991",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75365673-06ed-42cf-8cbb-cba8e0aecb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count vehicles needing renewal in 2025\n",
    "renewals_2025 = df[df['insuranceYear'] == 2025].shape[0]\n",
    "print(\"Number of vehicles needing renewal in 2025:\", renewals_2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ee90d3-ae5f-498f-ab2c-dbc1cf235ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db74c45b-c77f-4b11-be67-c4d2cfcd886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['insuranceValidity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17e07f1-fe6a-4a8d-8b77-e4896fc64bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bac6812-a69c-422b-9257-283ed932fbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef36aa28-eaa1-4444-9419-dcc7b111ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Plot for visualize outliers\n",
    "columns = df.select_dtypes(include = ['number'])\n",
    "\n",
    "for col in columns.columns:\n",
    "    plt.figure(figsize=(10,6))\n",
    "    columns.boxplot(column = col)\n",
    "    plt.title(f\" Box Plot for {col}\")\n",
    "    plt.ylabel('Values')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5891fdb-c07b-4e10-ab84-171206c1b32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier's removel with IQR\n",
    "def handle_outlier(df):\n",
    "    for col in df.select_dtypes(include = ['int64','float64']).columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        lower = Q1 - 1.5 *IQR\n",
    "        upper = Q3 + 1.5 *IQR\n",
    "\n",
    "        df[col] = df[col].apply(lambda x: \n",
    "        lower if x<lower else \n",
    "        upper if x>upper else\n",
    "        x)\n",
    "    return df\n",
    "df1 = handle_outlier(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3764cda-2b71-4b3d-8829-6337c762ae81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Box-Plot after removing outliers\n",
    "column = df1.select_dtypes(include=['int64','float64'])\n",
    "\n",
    "for col in column.columns:\n",
    "    plt.figure()\n",
    "    column.boxplot(column = col)\n",
    "    plt.title(f\"Box Plot for {col} after IQR\")\n",
    "    plt.ylabel('Value')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b024a8-e756-40a7-8989-69b03b90c2c2",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff33e948-3ed9-4b40-8f3f-262b0d8436c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a23c21a-d41d-4d66-9238-be504f62146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie Chart - Distribution of vehicle categories\n",
    "plt.figure(figsize=(8, 8))\n",
    "df['category'].value_counts().plot.pie(autopct='%1.1f%%', cmap='coolwarm')\n",
    "plt.title(\"Vehicle Category Distribution\")\n",
    "plt.ylabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f291487-5a10-472f-93ea-6dfb33fe8d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Plot - Top 10 vehicle makers\n",
    "top_makers = df['makerName'].value_counts().head(10)\n",
    "top_makers.plot(kind='bar', color='skyblue')\n",
    "plt.title(\"Top 10 Vehicle Makers\")\n",
    "plt.xlabel(\"Maker Name\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b268219-804d-4670-a699-a871ebd935f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for Insurance Year Distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['insuranceYear'].dropna(), bins=20, kde=False, color='blue')\n",
    "plt.title(\"Insurance Year Distribution (Histogram)\")\n",
    "plt.xlabel(\"Insurance Year\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586f4e3d-78a3-4e9e-b704-30396a27ecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel Density Estimation (KDE) for Insurance Year\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(df['insuranceYear'].dropna(), fill=True, color='green')\n",
    "plt.title(\"Kernel Density Estimation (KDE) for Insurance Year\")\n",
    "plt.xlabel(\"Insurance Year\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c564b0d-c03d-4119-beea-cfac1e983fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate heatmap for numerical columns only\n",
    "numerical_df = df.select_dtypes(include=['number'])  # Select only numeric columns\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(numerical_df.corr(), annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc646751-9300-40b5-9992-1fbfd1d2644b",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6db767f-558a-4afa-82ac-9ad73255a92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and testing sets\n",
    "X = df1.drop('insuranceYear', axis=1)\n",
    "y = df1['insuranceYear']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0ab1c7-b559-4830-a6b5-b6f09011bfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "categorical_columns = ['vehicleClass', 'secondVehicle', 'category', 'OfficeCd', 'fromdate', 'todate', 'makerName', 'modelDesc', 'makeYear', 'tempRegistrationNumber']  # List your categorical columns\n",
    "\n",
    "# Apply Label Encoding to each categorical column\n",
    "encoder = LabelEncoder()\n",
    "for col in categorical_columns:\n",
    "    X[col] = encoder.fit_transform(X[col])\n",
    "\n",
    "# Check transformed data\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67912419-936f-410a-8686-344893865770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datetime columns to numerical values\n",
    "def preprocess_data(X):\n",
    "    for col in X.select_dtypes(include=['datetime64']):  # Identify datetime columns\n",
    "        X[col] = X[col].astype('int64') // 10**9  # Convert to Unix timestamp (seconds)\n",
    "    return X\n",
    "\n",
    "# Function to select best K features for regression\n",
    "def select_best_features(X, y, K=10):\n",
    "    X = preprocess_data(X)  # Convert datetime columns\n",
    "    selector = SelectKBest(score_func=f_regression, k=K)  # Use f_regression for regression problems\n",
    "    X_new = selector.fit_transform(X, y)  # Apply feature selection\n",
    "    \n",
    "    # Get feature scores\n",
    "    feature_scores = pd.DataFrame({'Feature': X.columns, 'Score': selector.scores_})\n",
    "    feature_scores = feature_scores.sort_values(by='Score', ascending=False)\n",
    "    \n",
    "    print(\"Top Selected Features:\\n\", feature_scores.head(K))  # Print selected features\n",
    "    \n",
    "    return X_new, feature_scores  # Return both selected features and scores\n",
    "\n",
    "# Set K (number of top features to select)\n",
    "K = 10\n",
    "\n",
    "# Preprocess and Apply feature selection\n",
    "X = preprocess_data(X)\n",
    "X_selected, feature_scores = select_best_features(X, y, K)\n",
    "\n",
    "# Print all feature scores (optional)\n",
    "print(\"\\nAll Feature Scores:\\n\", feature_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafeffb0-20f7-4ae5-91ac-fdf52d604525",
   "metadata": {},
   "source": [
    "# Split Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7d232b-5389-4ca3-b9cf-18d562d27818",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8573d76b-8993-4616-8f79-591ce1a58d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shapes of the resulting datasets\n",
    "print(\"Training feature set shape:\", X_train.shape)\n",
    "print(\"Testing feature set shape:\", X_test.shape)\n",
    "print(\"Training target set shape:\", y_train.shape)\n",
    "print(\"Testing target set shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27f343a-5c26-4808-8146-79ae24d9c769",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdff336-9a78-487e-8ee5-05904a3d7b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc29e243-59ea-4a17-86f8-116adac7cf31",
   "metadata": {},
   "source": [
    "# Build the ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45c5bf3-df70-40ee-8fe2-aefa0bb8de41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f531c4-33b1-4c53-8c24-184b150fc526",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    '1. Linear Regression' : LinearRegression(),\n",
    "    '2. Decision Tree Regressor' : DecisionTreeRegressor(),\n",
    "    '3. Random Forest Regressor' : RandomForestRegressor(),\n",
    "    '4. Gradient Boosting Regressor' : GradientBoostingRegressor(),\n",
    "    '5. Support Vector Regressor' : SVR(),\n",
    "    '6. Lasso Regression' : Lasso(),\n",
    "    '7. AdaBoost Regressor' : AdaBoostRegressor()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc5cd2e-0b2e-413a-b925-b0ebece82685",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa115c4-3652-47a2-86c2-d98389ab027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models\n",
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    results[model_name] = {'MSE': mse, 'MAE': mae, 'R² Score': r2, 'RMSE': rmse}\n",
    "\n",
    "# Convert results to DataFrame\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416530a6-16d6-42f4-aea3-364ada6a9916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding The Best Model\n",
    "best_model = results_df['R² Score'].idxmax()\n",
    "print(f'The best Model based on R2 Score is:\\n {best_model}')\n",
    "print(results_df.loc[best_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6365aa27-4bbb-406e-bfe4-4f172a366fbf",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974b0a79-ae9e-4f6c-b544-24299f86bd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': [None, 10,],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be309de-0ce9-472d-8df5-e5658f7334ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Grid Search\n",
    "grid_search = GridSearchCV(estimator=RandomForestRegressor(random_state=42), param_grid=param_grid, \n",
    "                           cv=5, scoring='neg_mean_squared_error',verbose =1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400133e6-ee22-48aa-842e-178a16d69e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Grid Search to the data\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_param = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Get the best parameters and score\n",
    "print(f\"Best Parameters: {best_param}\")\n",
    "print(f\"Best Cross-Validation MSE Score: {-best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f290e71-bbd1-47f4-b2d8-2137701f9451",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_test_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_mae = mean_squared_error(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "# Printing the results\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Test R2: {test_r2}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964346e2-163c-4e09-96a0-0e58ef6c944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Model Performance with Residual Analysis\n",
    "\n",
    "# Predicting values  \n",
    "y_train_pred = best_model.predict(X_train_scaled)  \n",
    "residuals = y_train - y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc24aebb-1044-4dc6-ae42-44ca72001b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validate the model  \n",
    "cv_scores = cross_val_score(best_model, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')  \n",
    "print(f'Cross-Validation MSE: {-cv_scores.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b33610c-d784-412c-ac20-b470ff512342",
   "metadata": {},
   "source": [
    "# Pipeline Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eb30d4-fb6b-482b-a40a-0ea18282050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting data into X and y\n",
    "X = df1.drop(['insuranceYear'], axis=1)\n",
    "y = df1['insuranceYear']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51d1861-9e69-4d8a-9c4e-dcceb2568a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c33c3d-9e4b-4412-aea7-910c85862cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Separate numeric and non-numeric columns\n",
    "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Preprocessing for numeric data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Final pipeline with the regressor\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(\n",
    "        n_estimators=50,              # Best number of estimators from GridSearchCV\n",
    "        min_samples_split=5,          # Best min_samples_split from GridSearchCV\n",
    "        min_samples_leaf=2,           # Best min_samples_leaf from GridSearchCV\n",
    "        max_depth=10,                 # Best max_depth from GridSearchCV\n",
    "        random_state=42))])           # Ensure reproducibility\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec67f59-10a1-461b-94f5-1c567c15bd9d",
   "metadata": {},
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ab2ffc-8328-489b-ab49-1ab60b7804e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire pipeline\n",
    "joblib.dump(pipeline,'random_forest_pipeline.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c0220d-f67c-461d-aeab-1e0761ba9ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_pipeline = joblib.load('random_forest_pipeline.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48f2714-786f-48c4-a631-c57771c79abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "53bfaaeb-4e98-4c7d-9e27-5de55dd6b1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unseen dataset created and saved as 'unseen_data.csv'\n"
     ]
    }
   ],
   "source": [
    "# Select first 5,000 rows from original dataset\n",
    "unseen_data = df.sample(n=5000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save unseen data to CSV\n",
    "unseen_data.to_csv(\"unseen_data.csv\", index=False)\n",
    "\n",
    "print(\"Unseen dataset created and saved as 'unseen_data.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "1353af89-c910-4639-be75-5c79149faa6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unseen data successfully loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load unseen dataset \n",
    "unseen_data_df = pd.read_csv(\"unseen_data.csv\")\n",
    "print(\"Unseen data successfully loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f500e270-c3c4-4fdf-92be-97956370348c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of Unseen Data:\n",
      "    slno                              modelDesc vehicleClass    makeYear  \\\n",
      "0  60769             TVS - JUPITER 125 BSVI-PH2  MOTOR CYCLE  01/01/2025   \n",
      "1   3188  INNOVA CRYSTA 2.4Z (MT) (7S) BSVI-PH2    MOTOR CAR  01/11/2024   \n",
      "2  22289                SHINE 125 DISK BSVI-PH2  MOTOR CYCLE  01/08/2024   \n",
      "3  41692              ACTIVA 125 DISC. BSVI-PH2  MOTOR CYCLE  01/01/2025   \n",
      "4  40024              SPLENDOR+ -(DRS) BSVI-PH2  MOTOR CYCLE  01/11/2024   \n",
      "\n",
      "  insuranceValidity secondVehicle tempRegistrationNumber       category  \\\n",
      "0        2030-01-27             N            TG02CTR6572  Non Transport   \n",
      "1        2028-01-01             Y            TG03BTR0063  Non Transport   \n",
      "2        2030-01-08             N            TG02CTR5145  Non Transport   \n",
      "3        2030-01-19             Y            TG12ATR8873  Non Transport   \n",
      "4        2030-01-18             N             TG31TR3822  Non Transport   \n",
      "\n",
      "                        makerName          OfficeCd    fromdate      todate  \\\n",
      "0           TVS MOTOR COMPANY LTD    RTA PEDDAPALLI  28/01/2025  28/01/2025   \n",
      "1  TOYOTA KIRLOSKAR MOTOR PVT LTD   RTA HANUMAKONDA  02/01/2025  02/01/2025   \n",
      "2  HONDA MOTORCYCLE&SCOOTER(I)P L   RTA JAYASHANKAR  10/01/2025  10/01/2025   \n",
      "3  HONDA MOTORCYCLE&SCOOTER(I)P L  RTA-HYDERABAD-EZ  20/01/2025  20/01/2025   \n",
      "4               HERO MOTOCORP LTD  RTA NAGARKURNOOL  19/01/2025  19/01/2025   \n",
      "\n",
      "   insuranceYear  \n",
      "0           2030  \n",
      "1           2028  \n",
      "2           2030  \n",
      "3           2030  \n",
      "4           2030  \n"
     ]
    }
   ],
   "source": [
    "# Check Unseen data to confirm data is loaded\n",
    "print(\"Preview of Unseen Data:\")\n",
    "print(unseen_data_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "cec2096b-8bad-4096-b3e8-0c9b8351a580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop target column if present (ensure we are not using labels)\n",
    "if 'insuranceYear' in unseen_data_df.columns:\n",
    "    unseen_X = unseen_data_df.drop(columns=['insuranceYear'])\n",
    "else:\n",
    "    unseen_X = unseen_data_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "9a3b1ffc-a42b-4d2d-9a5e-b0bc2d9e215f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline successfully loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load the saved pipeline (Check if the file exists)\n",
    "try:\n",
    "    loaded_pipeline = joblib.load(\"random_forest_pipeline.joblib\")\n",
    "    print(\"Pipeline successfully loaded!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The file 'risk_malware_pipeline.pkl' was not found. Please check the file path.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6f693783-0ade-4f46-99b7-145a1260d449",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Make predictions on unseen data\n",
    "unseen_predictions = loaded_pipeline.predict(unseen_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "37731f65-f915-4f1e-9d91-21c0ad231855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to the dataset\n",
    "unseen_data_df['Predicted_Insurance_Year'] = unseen_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "cf2b6f3f-4486-45e9-be73-504eefc4156b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unseen data saved as 'unseen_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "# Save the results\n",
    "unseen_data_df.to_csv(\"unseen_predictions.csv\", index=False)\n",
    "\n",
    "print(\"unseen data saved as 'unseen_predictions.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "22ce2718-0e42-498d-9341-790ae4877870",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'regression_report' from 'sklearn.metrics' (C:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[198], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, regression_report\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Check if actual labels are present in unseen dataset\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minsuranceYear\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m unseen_data_df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Extract actual labels\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'regression_report' from 'sklearn.metrics' (C:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, regression_report\n",
    "\n",
    "# Check if actual labels are present in unseen dataset\n",
    "if 'insuranceYear' in unseen_data_df.columns:\n",
    "    # Extract actual labels\n",
    "    actual_labels = unseen_data_df['insuranceYear']\n",
    "    \n",
    "    # Evaluate model performance\n",
    "    accuracy = accuracy_score(actual_labels, unseen_predictions)\n",
    "    print(f\"Accuracy on Unseen Data: {accuracy:.4f}\\n\")\n",
    "\n",
    "    print(\"Regression Report:\")\n",
    "    print(regression_report(actual_labels, unseen_predictions))\n",
    "\n",
    "else:\n",
    "    print(\"No actual labels found in the unseen dataset. Evaluation cannot be performed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5184c2-0c77-4f80-a481-ad2455ee1256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
